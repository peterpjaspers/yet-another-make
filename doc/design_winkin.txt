-- Winkin and winkin cache
Winkin is a build avoidance strategy that tries to re-use (winkin) files
generated by previously builds.

-- Winkin cache
A build searches for suitable outputs in the winkin cache. If it finds them
then it re-uses them, else it generates the outputs in the local worktree and
adds them to the winkin cache.

-- Winkin criteria (when can a file in the winkin cache be re-used)
A file G generated by a build in some worktree SRC and stored in the winkin
cache can be winked-in (re-used) from the cache by a build in worktree DST
when:
    - the build scripts to generate G in DST and SRC are equal
    - AND DST and SRC have the same output file path names
    - AND all input files of G in SRC exist and are identical to the ones
      in DST, i.e. the input file aspect hashes are identical. 
Note: input files can be both source files and generated files.
Note: winkin can be implemented in various ways. The most simple one is
to copy G from winkin cache to DST worktree. Other implementations may 
create a link from DST to G in winkin cache or use a virtual filessystem
to make the cached G visible in SRC.

-- Non-deterministic hashes cause sub-optimal winkin
A problem in dealing with generated input files is that some tools, e.g. the
Microsoft compilers and linkers, are not deterministic: running these tools 
twice on identical inputs produces outputs that are functionally identical
but not binary identical. This behavior will cause sub-optimal winkin unless
the input file aspect hashes exclude non-deterministic parts of the file.

-- Example of sub-optimal winkin due to non-deterministic hashes
Assume A.dll is linked from object file S.obj, S.obj is compiled from S.cpp, 
S.cpp is identical in SRC and DST, including equality of all (recursively) 
included header files. 
Further assume that A.dll and S.obj have been built in SRC and are stored in 
the winkin cache and that only S.obj has been built locally (i.e. not 
winked-in) in DST.
Assume that the used compiler is non-deterministic, i.e. SRC/S.obj and 
DST/S.obj are not binary identical. If the file aspect hashes do not exclude
the non-deterministic parts of the object files then the hashes of SRC/S.obj 
and DST/S.obj will be different and the winkin criteria for A.dll are not met.
Pseudo code:
    - void Node::pendingStartSelf() {
          // return whether output is out-dated
          // selfExecute to be executed (async in thread pool) when output
          // is out-dated.
          return _executionHash != computeExecutionHash()
      }
    - void selfExecute() {
          if (outputFile cannot be winked-in from cache) {
              execute build script (i.e. (re-)generate outputFile)
          } else {
              copy SRC/outputFile to DST/outputFile // winkin
          }
      }

-- Work-around in mrmake for winkin in case of non-deterministic hashes
The problem is worked-around by mrmake's forcedWinkin option.
This option forces winkin-able files to be winked-in (copied), even when the
file was already up-to-date. 
Build logic:
    - at start of build: 
        if (forcedWinkin) {
            set all nodes in the graph rooted by A.dll to Dirty.
            // I.e. set A.dll and S.obj to Dirty.
            // Note: pendingStartSelf() is only called on Dirty nodes
        }
    - bool Node::pendingStartSelf() {
          return forcedWinkin || _executionHash != computeExecutionHash()
          // I.e. always selfExecute in case of forcedWinkin
      }
    - void selfExecute() {
          if (outputFile cannot be winked-in from cache) {
              execute build script (i.e. (re-)generate outputFile)
          } else {
              if (outputFile is not up-to-date) {
                  copy SRC/outputFile to DST/outputFile // winkin
              } else if (
                  forcedWinkin 
                  && (hashOf(SRC/outputFile) != hashOf(DST/outputFile)
              ) {
                  copy SRC/outputFile to DST/outputFile // force winkin
              }
          }
      }
 
Note that forcedWinkin adds some overhead because already up-to-date nodes 
are re-visited.

-- mrmake winkin versus yam winkin
mrmake winkin has some disadvantages and inconveniences:
    - the winkin cache is a worktree. So content of cache is limited to 
      the latest build in that worktre).
    - engineer must manage the cache SRC worktree, i.e. not delete it, and
      get/keep it in built state.
    - mrmake only performs winkin builds on request by engineer. Engineer has 
      to specify the SRC worktree on mrmake command line.
    - mrmake can only winkin from 1 SRC worktree at-a-time.

Ideal situation:
    - to perform winkin builds by default.
    - to winkin from build results from all builds from all worktrees from  all
      users from all (?) repositories.

Practical problems:
    - storage space is limited.
    - winkin time complexity for file G is O(N), where N is the number of 
      cached versions of G.
    - concurrent access to cache.

Possible ways to manage this:
    - only cache build results from designated worktrees/builds.
    - auto-delete cached build results that have not been re-used for a while.
    - todo: how to handle concurrency (including auto-delete use-case)?
    - todo: is it possible to reduce O(N)?

-- O(1) winkin in Bazel (Google's open source build system)
Yam requires the user to specify in the build file for each command C
    - all prerequisites of C
      Prerequisites are commands that generate files needed as input by the
      C. Hence prerequisites must execute before C can execute. 
    - all output files of C (inspired by tup)

Note: unlike Bazel, yam does not require specification of all inputs of C.
Example:
Let C be a command that compiles the C++ source file XYZ.cpp.
XYZ.cpp includes header files, which may include other header files.
Bazel requires XYZ.cpp and all of its (recursively) included headers to be 
specified as inputs of C. Bazel will only re-compile XYZ.cpp when one or more
of the specified inputs is modified. Note that this may cause issues:
    - unnecessary re-compilation when the input set is over-specified
    - and, more seriously, not-performed necessary re-compilation when the 
      input set is under-specified.
Yam on the other hand monitors all file access done by C to detect the set of
inputs and outputs of C. Yam will only re-compile when one or more files in this
set of in/outputs is modified. Yam thus guarantees correct and efficient
incremental build.

Note: Knowing the outputs of a command allows yam to exercise full control over
the set of generated files. E.g. yam deletes a previously built output files
when those outputs are removed from the build file, thus eliminating the risk of 
using stale output files.

The Bazel approach however also has advantages:
    - sandbox build. A sandbox build runs in an environment (the sandbox) that
      only contains the declared inputs. A successfull sandbox build proves 
      reproducability of the build.
    - O(1) winkin overhead

The sandbox advantage is small because yam can support it as follows:
    - first run a normal build. This results in a cmd node graph that contains
      all inputs (and outputs)
    - then run a sandbox build. This will first copy all inputs to the sandbox
      and then run a normal build in that sandbox. 
      
The O(1) winkin overhead however cannot be realized by yam.
The O(1) winkin overhead in Bazel can be realized as follows:

Store the build output of a cmd node as follows:
    - compute the so-called winkin hash (my term, not Bazel's) of a cmd node:
        XXH64_hash_t CommandNode::computeWinkinHash() {
            return computeHash(
                computeHash(_script), 
                computeHash(getNamesOf(_outputFiles)), 
                computeHash(getHashesOf(_inputFiles)));
    - store the cmd output files in the winkin cache in a location identified
      by the winkin hash
    - the cache supports O(1) lookup of the output files location by means of a
      lookup table {winkinHash => output files location}

When building an out-dated cmd node find winkin-able files:
    - compute the winkin hash of the out-dated cmd node
    - lookup the cached output files location by winkin hash
      if (output file location found) {
            re-use the files from the found location
      } else {
            execute cmd script
      }


-- O(N) winkin in yam
As we have seen in the Bazel winkin discussion O(1) overhead requires
a-priori knowlegde of all inputs. Yam however does not have this a-priori 
knowledge because inputs are discovered during cmd execution. Therefore yam
can only realize O(N) overhead where N is the number of cached versions
of a cmd. Cached versions differ in cmd script and/or inputs and/or
outputs names and/or input versions (i.e. input file aspect hashes).

For yam the caching can be implemented as described below.

Store the build output of a cmd node in the winkin cache.
    - metaData contains: metaDataHash, inputFilesHash, executionHash
        and inputFileNames.
    - metaData.metaDataHash = cmd.computeMetaDataHash()
    - XXH64_hash_t CommandNode::computeMetaDataHash() {
          return computeHash(
              computeHash(_script),  
              computeHash(getNamesOf(_outputFiles)));
      }
    - metaData.inputFilesHash = cmd.computeInputFilesHash()
    - XXH64_hash_t CommandNode::computeInputsHash() {
          return computeHash(getHashesOf(_inputFiles));
      }
    - metaData.executionHash = cmd._executionHash.
        See design_computeExecutionHash.txt for details on how the
        executionHash is computed.
    - metaData.inputFilenames = cmd.inputFileNames
    - winkinCache contains a O(1) lookup table for metaData storage.
      winkinCache.metaData {metaDataHash => setOfMetaData} where
      setOfMetaData is a O(1) lookup table {executionHash => metaData}
    - winkinCache contains a O(1) lookup table for outputFiles storage.
      winkinCache.outputFiles {executionHash => set of output files}
    - void storeInWinkinCache(cmd) {
          metaData = createMetaDataFrom(cmd)
          setOfMetaData = winkinCache.metaData[metaData.metaDataHash]
          if (setOfMetaData.contains(metaData.executionHash)) {
              // do not store duplicate cmd versions
          } else {
              setOfMetaData[metaData.executionHash] = metaData
              winkinCache.outputFiles[metaData.executionHash] = copyOf(cmd.outputFiles)
          }
      }
        
Building an out-dated cmd node with winkin:
    void CommandNode::executeSelf() override {
        metaDataHash = computeMetaDataHash()
        metaDataSet = winkinCache.metaData[metaDataHash].values()
        if (metaDataSet == null) || !winkinFrom(metaDataSet))  
            execute cmd script
        }

    // Try to winkin from given set of cached cmd versions.
    // Return whether winkin is performed.
    bool winkinFrom(metaDataSet) {
        foreach metaData in metaDataSet {
            // We already know from metaDataHash that the cached cmd and the
            // out-dated cmd have same script and same set of outputfiles.
            inputFilesHash = computeHash(computeFileHashes(metaData._inputFileNames))
            if (inputsFileHash == metadata.inputsFileHash) {
                // We now also know that the input files of the cached cmd 
                // exist in the worktree of the out-dated cmd and have same 
                // hashes. Hence we can re-use the output files of the cached cmd.
                outdatedCmd.outputFiles = winkinCache.outputFiles[metadata.executionHash]
                return true;
            }
        }
        return false;
    }
  
  Some attention points:
    - computeFileHashes computes the input file aspect hashes
    - cached cmd may have used different input aspects and/or hash 
      implementations than out-dated cmd. Probably not a problem
      because in that cases the inputFilesHash will not match and
      out-dated cmd script be executed.
